{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9dd95279",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "005672a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comment this out, the next box below is more dynamic\n",
    "# import sys\n",
    "# sys.path.append('/mnt/batch/tasks/shared/LS_root/mounts/clusters/aml-scoring-vm5/code/Users/0szeng/gitrepos/LifeCycle/Lib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b4e95f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# 获取 Lifecycle 的路径（Lib 的上一级）\n",
    "upper_level_folder = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "sys.path.append(upper_level_folder)\n",
    "\n",
    "# 现在可以像导入包一样导入 Lib\n",
    "# from Lib import aml_datalake_loader, logger\n",
    "from Lib import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c3cd4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no need for below because of __init__.py in Lib folder\n",
    "# from Lib.logger import Logger\n",
    "# from Lib.connectors import DataConnector\n",
    "# from Lib.aml_datalake_loader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a6e450",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:azure.identity._credentials.environment:No environment configuration found.\n",
      "INFO:azure.identity._credentials.managed_identity:ManagedIdentityCredential will use Azure ML managed identity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:azure.identity._credentials.environment:No environment configuration found.\n",
      "INFO:azure.identity._credentials.managed_identity:ManagedIdentityCredential will use Azure ML managed identity\n",
      "WARNING:opentelemetry.trace:Overriding of current TracerProvider is not allowed\n",
      "WARNING:opentelemetry._logs._internal:Overriding of current LoggerProvider is not allowed\n",
      "WARNING:opentelemetry.metrics._internal:Overriding of current MeterProvider is not allowed\n",
      "WARNING:opentelemetry.instrumentation.instrumentor:Attempting to instrument while already instrumented\n",
      "WARNING:opentelemetry.instrumentation.instrumentor:Attempting to instrument while already instrumented\n",
      "WARNING:opentelemetry.instrumentation.instrumentor:Attempting to instrument while already instrumented\n",
      "WARNING:opentelemetry.instrumentation.instrumentor:Attempting to instrument while already instrumented\n",
      "WARNING:opentelemetry.instrumentation.instrumentor:Attempting to instrument while already instrumented\n",
      "INFO:azure.identity._credentials.chained:DefaultAzureCredential acquired a token from ManagedIdentityCredential\n",
      "INFO:azure.identity._internal.msal_managed_identity_client:AzureMLCredential.get_token_info succeeded\n",
      "INFO:azure.identity._internal.decorators:ManagedIdentityCredential.get_token_info succeeded\n",
      "INFO:azure.identity._credentials.default:DefaultAzureCredential acquired a token from ManagedIdentityCredential\n",
      "WARNING:opentelemetry.sdk._logs._internal.export:Timeout was exceeded in force_flush().\n",
      "INFO:azure.monitor.opentelemetry.exporter.export._base:Transmission succeeded: Item received: 1. Items accepted: 1\n",
      "INFO:azure.monitor.opentelemetry.exporter.export._base:Transmission succeeded: Item received: 4. Items accepted: 4\n",
      "INFO:azure.identity._credentials.environment:No environment configuration found.\n",
      "INFO:azure.identity._credentials.managed_identity:ManagedIdentityCredential will use Azure ML managed identity\n",
      "WARNING:opentelemetry.trace:Overriding of current TracerProvider is not allowed\n",
      "WARNING:opentelemetry._logs._internal:Overriding of current LoggerProvider is not allowed\n",
      "WARNING:opentelemetry.metrics._internal:Overriding of current MeterProvider is not allowed\n",
      "WARNING:opentelemetry.instrumentation.instrumentor:Attempting to instrument while already instrumented\n",
      "WARNING:opentelemetry.instrumentation.instrumentor:Attempting to instrument while already instrumented\n",
      "WARNING:opentelemetry.instrumentation.instrumentor:Attempting to instrument while already instrumented\n",
      "WARNING:opentelemetry.instrumentation.instrumentor:Attempting to instrument while already instrumented\n",
      "WARNING:opentelemetry.instrumentation.instrumentor:Attempting to instrument while already instrumented\n",
      "INFO:azure.identity._credentials.chained:DefaultAzureCredential acquired a token from ManagedIdentityCredential\n",
      "INFO:azure.identity._internal.msal_managed_identity_client:AzureMLCredential.get_token_info succeeded\n",
      "INFO:azure.identity._internal.decorators:ManagedIdentityCredential.get_token_info succeeded\n",
      "INFO:azure.identity._credentials.default:DefaultAzureCredential acquired a token from ManagedIdentityCredential\n",
      "/mnt/batch/tasks/shared/LS_root/mounts/clusters/aml-scoring-vm5/code/Users/0szeng/gitrepos/AzureMachineLearning/LifeCycle/Lib/aml_datalake_loader.py:65: DtypeWarning: Columns (0,2,4,7,8,9,12,14,17,19,21,22,24,25,28,30,33,36,38,41,48,54,55,56,64,65,66,67) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(input_data.path, header=None)\n",
      "INFO:Lib.aml_datalake_loader:Loaded 47312 rows from Data Lake path: azureml://subscriptions/24fceea3-b944-4568-9028-d77c36beaab5/resourcegroups/rg-machinelearning-prod-ae-001/workspaces/arriba-mlworkspace-prod-ae-001/datastores/stdataanalyticsadls001/paths/WilsonAI/claim_rollup.csv\n",
      "INFO:Lib.aml_datalake_loader:Saved DataFrame to /home/azureuser/mycode/gitrepos/AzureMachineLearning/LifeCycle/Scripts/dataprep_input/claim_rollup.csv\n",
      "INFO:azure.identity._credentials.environment:No environment configuration found.\n",
      "INFO:azure.identity._credentials.managed_identity:ManagedIdentityCredential will use Azure ML managed identity\n",
      "WARNING:opentelemetry.trace:Overriding of current TracerProvider is not allowed\n",
      "WARNING:opentelemetry._logs._internal:Overriding of current LoggerProvider is not allowed\n",
      "WARNING:opentelemetry.metrics._internal:Overriding of current MeterProvider is not allowed\n",
      "WARNING:opentelemetry.instrumentation.instrumentor:Attempting to instrument while already instrumented\n",
      "WARNING:opentelemetry.instrumentation.instrumentor:Attempting to instrument while already instrumented\n",
      "WARNING:opentelemetry.instrumentation.instrumentor:Attempting to instrument while already instrumented\n",
      "WARNING:opentelemetry.instrumentation.instrumentor:Attempting to instrument while already instrumented\n",
      "WARNING:opentelemetry.instrumentation.instrumentor:Attempting to instrument while already instrumented\n",
      "INFO:azure.monitor.opentelemetry.exporter.export._base:Transmission succeeded: Item received: 3. Items accepted: 3\n",
      "INFO:azure.monitor.opentelemetry.exporter.export._base:Transmission succeeded: Item received: 10. Items accepted: 10\n",
      "/mnt/batch/tasks/shared/LS_root/mounts/clusters/aml-scoring-vm5/code/Users/0szeng/gitrepos/AzureMachineLearning/LifeCycle/Lib/aml_datalake_loader.py:65: DtypeWarning: Columns (0,1,2,4,5,6,7,8,9,10,11,13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(input_data.path, header=None)\n",
      "INFO:Lib.aml_datalake_loader:Loaded 3356012 rows from Data Lake path: azureml://subscriptions/24fceea3-b944-4568-9028-d77c36beaab5/resourcegroups/rg-machinelearning-prod-ae-001/workspaces/arriba-mlworkspace-prod-ae-001/datastores/stdataanalyticsadls001/paths/WilsonAI/case_bill.csv\n",
      "INFO:azure.monitor.opentelemetry.exporter.export._base:Transmission succeeded: Item received: 2. Items accepted: 2\n",
      "INFO:Lib.aml_datalake_loader:Saved DataFrame to /home/azureuser/mycode/gitrepos/AzureMachineLearning/LifeCycle/Scripts/dataprep_input/case_bill.csv\n",
      "INFO:azure.identity._credentials.environment:No environment configuration found.\n",
      "INFO:azure.identity._credentials.managed_identity:ManagedIdentityCredential will use Azure ML managed identity\n",
      "WARNING:opentelemetry.trace:Overriding of current TracerProvider is not allowed\n",
      "WARNING:opentelemetry._logs._internal:Overriding of current LoggerProvider is not allowed\n",
      "WARNING:opentelemetry.metrics._internal:Overriding of current MeterProvider is not allowed\n",
      "WARNING:opentelemetry.instrumentation.instrumentor:Attempting to instrument while already instrumented\n",
      "WARNING:opentelemetry.instrumentation.instrumentor:Attempting to instrument while already instrumented\n",
      "WARNING:opentelemetry.instrumentation.instrumentor:Attempting to instrument while already instrumented\n",
      "WARNING:opentelemetry.instrumentation.instrumentor:Attempting to instrument while already instrumented\n",
      "WARNING:opentelemetry.instrumentation.instrumentor:Attempting to instrument while already instrumented\n",
      "INFO:Lib.aml_datalake_loader:Loaded 94785 rows from Data Lake path: azureml://subscriptions/24fceea3-b944-4568-9028-d77c36beaab5/resourcegroups/rg-machinelearning-prod-ae-001/workspaces/arriba-mlworkspace-prod-ae-001/datastores/stdataanalyticsadls001/paths/WilsonAI/claim_rollup_mapping.csv\n",
      "INFO:Lib.aml_datalake_loader:Saved DataFrame to /home/azureuser/mycode/gitrepos/AzureMachineLearning/LifeCycle/Scripts/dataprep_input/claim_rollup_mapping.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:azure.monitor.opentelemetry.exporter.export._base:Transmission succeeded: Item received: 7. Items accepted: 7\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# from aml_datalake_loader import DataLoader\n",
    "\n",
    "# 拆分路径和文件名\n",
    "path_n = '/home/azureuser/mycode/gitrepos/AzureMachineLearning/LifeCycle/Scripts'\n",
    "folder_n = 'dataprep_input'\n",
    "filenames = ['claim_rollup.csv', 'case_bill.csv', 'claim_rollup_mapping.csv']\n",
    "\n",
    "# 拼接输出路径\n",
    "output_dir = os.path.join(path_n, folder_n)\n",
    "\n",
    "# 初始化并加载数据\n",
    "for filename in filenames:\n",
    "    loader = DataLoader(source='datalake', filename=filename, output_dir=output_dir)\n",
    "    loader.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "788adfcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_local_data = True\n",
    "exclude_travel = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8847956b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weekly_estimates(logger, cost, claims, exclude_travel):\n",
    "\n",
    "    logger.debug('Getting weekly estimates', True)\n",
    "\n",
    "    max_weeks = 20\n",
    "\n",
    "    exclude_types = ['Travel', 'Reporting']\n",
    "\n",
    "    # exclude travel activities if required\n",
    "    if exclude_travel:\n",
    "        logger.debug('Removing travel activities')\n",
    "        cost = cost[~cost['type'].isin(exclude_types)]\n",
    "\n",
    "    # Prepare claim dates\n",
    "    claims_df = claims[['ClaimNo', 'first_referral', 'DateClosedLast']]\n",
    "    claims_df['first_referral'] = pd.to_datetime(claims_df['first_referral'])\n",
    "    claims_df['date_closed'] = pd.to_datetime(claims_df['DateClosedLast'])\n",
    "\n",
    "    # Work out day differences from date of referral\n",
    "    activity_df = cost[cost['BillDate'].notnull()]\n",
    "    activity_df = activity_df.merge(claims_df[['ClaimNo', 'first_referral']], on='ClaimNo')\n",
    "    activity_df['activity_date'] = pd.to_datetime(activity_df['BillDate'].str[:10])\n",
    "    activity_df['diff_days'] = (activity_df['activity_date'] - activity_df['first_referral']).dt.days\n",
    "    activity_df = activity_df[activity_df['diff_days']>=0]\n",
    "\n",
    "    # Get weekly diffs and aggregate\n",
    "    activity_df['diff_weeks'] = (np.floor(activity_df['diff_days']/7)+1).astype('int')\n",
    "    agg = {\n",
    "        'Id': 'count',\n",
    "        'CostsTotalExTax':'sum',\n",
    "        'Duration':'sum'\n",
    "    }\n",
    "    weekly_df = activity_df.groupby(['ClaimNo', 'diff_weeks'], as_index=False).agg(agg)\n",
    "    weekly_df.rename(columns={'Id': 'n_activities', 'CostsTotalExTax':'total_cost', 'Duration':'total_duration'}, inplace=True)\n",
    "\n",
    "    # Create a separate aggregation by cost type\n",
    "    weekly_df_by_type = activity_df.groupby(['ClaimNo', 'diff_weeks', 'type'], as_index=False).agg(agg)\n",
    "    weekly_df_by_type.rename(columns={'Id': 'n_activities', 'CostsTotalExTax':'total_cost', 'Duration':'total_duration'}, inplace=True)\n",
    "    weekly_df_by_type_pivot = pd.pivot(weekly_df_by_type, index=['ClaimNo', 'diff_weeks'], values=['n_activities', 'total_cost', 'total_duration'], columns=['type'])\n",
    "    weekly_df_by_type_pivot.fillna(0, inplace=True)    \n",
    "    weekly_df_by_type_pivot.columns = [':'.join(col) for col in weekly_df_by_type_pivot.columns]\n",
    "    weekly_df_by_type_pivot.reset_index(inplace=True)\n",
    "\n",
    "    # Create a DF will all possible week numbers and claim numbers\n",
    "    week_nos = pd.DataFrame(pd.Series(range(1, max_weeks)), columns=['week_num'])\n",
    "    claim_nos = pd.DataFrame(activity_df['ClaimNo'].unique(), columns=['ClaimNo'])\n",
    "    week_nos['join_col']=1\n",
    "    claim_nos['join_col']=1\n",
    "    claim_week_nos = claim_nos.merge(week_nos).drop(columns=['join_col'])\n",
    "\n",
    "    # remove weeks that are beyond the claim closed date\n",
    "    claims_df['date_closed'] = pd.to_datetime(claims_df['date_closed'].fillna(datetime.date.today()))\n",
    "    claims_df['diff_days'] = (claims_df['date_closed'] - claims_df['first_referral']).dt.days\n",
    "    claims_df['week_num_max'] = (np.floor(claims_df['diff_days']/7)+1).astype('int')\n",
    "    claim_week_nos = claim_week_nos.merge(claims_df[['ClaimNo', 'week_num_max']])    \n",
    "    claim_week_nos = claim_week_nos[claim_week_nos['week_num'] <= claim_week_nos['week_num_max']]\n",
    "\n",
    "    # left-join the actual data zero-ing the nulls\n",
    "    weekly_df_all = claim_week_nos.merge(weekly_df, how='left', left_on=['ClaimNo', 'week_num'], right_on=['ClaimNo', 'diff_weeks'])\n",
    "    weekly_df_all = weekly_df_all.merge(weekly_df_by_type_pivot, how='left', left_on=['ClaimNo', 'week_num'], right_on=['ClaimNo', 'diff_weeks'])\n",
    "    weekly_df_all.fillna(0.0, inplace=True)\n",
    "    \n",
    "    return weekly_df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4891706b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_milestones(logger, cost_weekly):\n",
    "\n",
    "    logger.debug('Generating milestones', True)\n",
    "\n",
    "    milestone_wks = [2,4,6,8,12,16,20]\n",
    "    active_mins = 10 # threshold for determinning that a week has been active (from analysis)\n",
    "    cost_weekly['active'] = cost_weekly['total_duration'] >= active_mins\n",
    "\n",
    "    milestone_dfs = []\n",
    "    for milestone in milestone_wks:\n",
    "        cost_weekly_subset = cost_weekly[(cost_weekly['week_num']<=milestone) & (cost_weekly['week_num_max']>=milestone)]\n",
    "        milestone_df = cost_weekly_subset.groupby('ClaimNo', as_index=False).sum()\n",
    "        milestone_df[f'activity_score{active_mins}'] = milestone_df['active']/milestone        \n",
    "\n",
    "        cols_to_keep = []\n",
    "        cols_to_keep_rename = []\n",
    "        for col in list(milestone_df.columns):\n",
    "            if 'n_activities' in col or 'total_cost' in col or 'total_duration' in col or 'activity_score' in col:\n",
    "                cols_to_keep.append(col)\n",
    "                cols_to_keep_rename.append(f'{col}_wk{milestone}')\n",
    "        \n",
    "        milestone_df.index = milestone_df['ClaimNo']\n",
    "        milestone_df = milestone_df[cols_to_keep]\n",
    "        milestone_df.columns = cols_to_keep_rename\n",
    "        milestone_dfs.append(milestone_df)\n",
    "\n",
    "    milestones_all = pd.concat(milestone_dfs, axis=1)\n",
    "    milestones_all['ClaimNo'] = milestones_all.index\n",
    "    milestones_all.reset_index(drop=True, inplace=True)\n",
    "    milestones_all.fillna(-1, inplace=True) # This signifies that the milestone is beyond when the case was closed\n",
    "\n",
    "    return milestones_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d33c6168",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_cost_type(desc):\n",
    "\n",
    "    desc = desc.lower()\n",
    "\n",
    "    if 'travel' in desc:\n",
    "        return 'Travel'\n",
    "\n",
    "    if 'report' in desc:\n",
    "        return 'Reporting'\n",
    "\n",
    "    if 'assessment' in desc:\n",
    "        return 'Assessment'\n",
    "\n",
    "    if 'communic' in desc:\n",
    "        if 'treat' in desc or 'health' in desc or 'medic' in desc:\n",
    "            return 'CommunicationTreating'\n",
    "        else:\n",
    "            return 'CommunicationOther'\n",
    "\n",
    "    if 'conferenc' in desc:\n",
    "        return 'CaseConference'\n",
    "\n",
    "    return 'Other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "643012fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorise_activities(logger, cost):\n",
    "\n",
    "    logger.debug('Categorising activities', True)\n",
    "\n",
    "    # the coding and classifications here are based on analysis conduced in May 2024.\n",
    "\n",
    "    # categorise from activity name first\n",
    "    cost['activity_lower'] = cost['ActivityName'].str.lower()\n",
    "    cost['activity_lower'].fillna('unknown', inplace=True)\n",
    "    cost['activity_grp'] = 'Other'\n",
    "    cost.loc[cost['activity_lower'].str.contains('training'), 'activity_grp'] = 'Coaching'\n",
    "    cost.loc[cost['activity_lower'].str.contains('coaching'), 'activity_grp'] = 'Coaching'\n",
    "    cost.loc[cost['activity_lower'].str.contains('conference'), 'activity_grp'] = 'Case Conference'\n",
    "    cost.loc[cost['activity_lower'].str.contains('liais'), 'activity_grp'] = 'Contact'\n",
    "    cost.loc[cost['activity_lower'].str.contains('phone'), 'activity_grp'] = 'Contact'\n",
    "    cost.loc[cost['activity_lower'].str.contains('email'), 'activity_grp'] = 'Contact'\n",
    "    cost.loc[cost['activity_lower'].str.contains('communic'), 'activity_grp'] = 'Contact'\n",
    "    cost.loc[cost['activity_lower'].str.contains('assessment'), 'activity_grp'] = 'Assessment'\n",
    "    cost.loc[cost['activity_lower'].str.contains('report'), 'activity_grp'] = 'Report'\n",
    "    cost.loc[cost['activity_lower'].str.contains('review'), 'activity_grp'] = 'Review'\n",
    "    cost.loc[cost['activity_lower'].str.contains('travel'), 'activity_grp'] = 'Travel'\n",
    "    \n",
    "    # now categorise from template name ---------------------------------------################################### comment-out below since no CaseBillTemplate\n",
    "    # cost['template_lower'] = cost['TemplateName'].str.lower()\n",
    "    # cost['template_lower'].fillna('unknown', inplace=True)\n",
    "    # cost['template_grp'] = 'Other'\n",
    "    # cost.loc[cost['template_lower'].str.contains('assessment'), 'template_grp'] = 'Assessment'\n",
    "    # cost.loc[cost['template_lower'].str.contains('training'), 'template_grp'] = 'Coaching'\n",
    "    # cost.loc[cost['template_lower'].str.contains('coaching'), 'template_grp'] = 'Coaching'\n",
    "    # cost.loc[cost['template_lower'].str.contains('conference'), 'template_grp'] = 'Case Conference'\n",
    "    # cost.loc[cost['template_lower'].str.contains('report'), 'template_grp'] = 'Report'\n",
    "    # cost.loc[cost['template_lower'].str.contains('review'), 'template_grp'] = 'Review'\n",
    "    # cost.loc[cost['template_lower'].str.contains('travel'), 'template_grp'] = 'Travel'\n",
    "    # cost.loc[cost['template_lower'].str.contains('contact'), 'template_grp'] = 'Contact'\n",
    "    \n",
    "    # and now combine them into a single classification\n",
    "    cost['type'] = cost['activity_grp']\n",
    "    # cost.loc[(cost['template_grp']=='Report') & (cost['activity_grp']=='Other'), 'type'] = 'Report' -----------------------  comment-out below since no CaseBillTemplate\n",
    "    # cost.loc[cost['template_grp'].isin(['Coaching', 'Case Conference', 'Travel', 'Review', 'Assessment']), 'type'] = cost['template_grp'] comment-out below since no CaseBillTemplate\n",
    "    cost.loc[cost['activity_grp']=='Travel', 'type'] = 'Travel'    \n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe05cc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_claim_totals(logger, claims_cost):\n",
    "\n",
    "    logger.debug('Removing claims that have fixed-fee costs')\n",
    "\n",
    "    # Figure out the total and % of the fixed-fee costs for each claim\n",
    "    all_costs = claims_cost.groupby('ClaimNo').agg({'CostsTotalExTax':'sum'})\n",
    "    claims_cases_cost_hourly = claims_cost[claims_cost['BillType']==1] #TODO: check this is still right\n",
    "    hourly_costs = claims_cases_cost_hourly.groupby('ClaimNo').agg({'CostsTotalExTax':'sum'})\n",
    "    hourly_costs.rename(columns={'CostsTotalExTax':'CostsTotalExTaxHourly'}, inplace=True)\n",
    "    all_costs = all_costs.merge(hourly_costs, left_index=True, right_index=True, how='left')\n",
    "    all_costs.fillna(0, inplace=True)\n",
    "    all_costs = all_costs[all_costs['CostsTotalExTax']>0]\n",
    "    all_costs['hourly_pctg'] = (all_costs['CostsTotalExTaxHourly'] / all_costs['CostsTotalExTax'])\n",
    "           \n",
    "    # Finalise DF\n",
    "    all_costs['ClaimNo'] = all_costs.index\n",
    "    all_costs.reset_index(drop=True, inplace=True)\n",
    "    all_costs = all_costs[['ClaimNo', 'CostsTotalExTax', 'hourly_pctg']].rename(columns={'CostsTotalExTax':'claim_total_cost', 'hourly_pctg': 'claim_cost_hourly_pctg'})\n",
    "    \n",
    "    return all_costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6c5cf14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prep():\n",
    "\n",
    "    logger = Logger()\n",
    "\n",
    "    with open('config.json', 'rb') as f:\n",
    "        config = json.load(f)\n",
    "\n",
    "    data_dir_in = config['data_folder_in']    \n",
    "    data_dir_out = config['data_folder_out']\n",
    "    ref_data_dir = config['ref_data_folder']\n",
    "\n",
    "    logger.debug('Starting lifecycle dataprep', True)\n",
    "\n",
    "    data_conn = DataConnector(logger, 'creds.json', data_dir_in, use_local_data, data_dir_out)    \n",
    "\n",
    "    claims = data_conn.read_data('claim_rollup', csv=True)\n",
    "    cost = data_conn.read_data('case_bill', csv=True)\n",
    "    claim_rollup_mapping = data_conn.read_data('claim_rollup_mapping', csv=True)\n",
    "     \n",
    "    # TODO: modify the ETL to include the template name in the case_bill extract, and the delete the below two lines\n",
    "    cost_templates = data_conn.read_data('CaseBillTemplate', csv=True)\n",
    "    #cost = cost[['Id']].merge(cost_templates[['Id', 'TemplateName', 'ActivityName', 'BillDate', 'CostsTotalExTax']], on='Id', how='left')\n",
    "    cost_templates['CostsTotalExTax'] = cost_templates['SubTotal'].fillna(0)/100\n",
    "    cost_templates['Duration'] = cost_templates['Minutes'].fillna(0)\n",
    "    #cost_templates = cost_templates[cost_templates['BillType']<3]\n",
    "    cost = cost_templates\n",
    "\n",
    "    cost = categorise_activities(logger, cost)\n",
    "    claims_cost = claim_rollup_mapping.merge(cost, how='left', on=['CaseServiceId'])\n",
    "    \n",
    "    claim_totals = get_claim_totals(logger, claims_cost)\n",
    "    claim_totals.to_csv(f'{data_dir_out}/claim_cost_totals.csv', index=False)\n",
    "\n",
    "    cost_weekly = get_weekly_estimates(logger, claims_cost, claims, exclude_travel)    \n",
    "    if exclude_travel:\n",
    "        cost_weekly.to_csv(f'{data_dir_out}/cost_weekly_ex_travel.csv', index=False)\n",
    "    else:\n",
    "        cost_weekly.to_csv(f'{data_dir_out}/cost_weekly.csv', index=False)\n",
    "\n",
    "    milestones = generate_milestones(logger, cost_weekly)\n",
    "\n",
    "    if exclude_travel:\n",
    "        milestones.to_csv(f'{data_dir_out}/milestones_ex_travel.csv', index=False)\n",
    "    else:\n",
    "        milestones.to_csv(f'{data_dir_out}/milestones.csv', index=False)\n",
    "\n",
    "    data_conn.close_connections()\n",
    "    \n",
    "    logger.debug('Completed lifcycle dataprep', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771011e4",
   "metadata": {},
   "source": [
    "#### below scripts replicate def data_prep():, but with breakdowns :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f83d3e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95m 2025-08-07 16:29:56 DEBUG Starting lifecycle dataprep \u001b[0m\n",
      "Claims shape: (47311, 68)\n",
      "Cost shape: (3356011, 14)\n",
      "Claim Rollup Mapping shape: (94784, 5)\n"
     ]
    }
   ],
   "source": [
    "def test_data_loading():\n",
    "    logger = Logger()\n",
    "    logger.debug('Starting lifecycle dataprep', True)\n",
    "\n",
    "    with open('config.json', 'r') as f:\n",
    "        config = json.load(f)\n",
    "\n",
    "    data_dir_in = config['data_folder_in']\n",
    "    data_dir_out = config['data_folder_out']\n",
    "\n",
    "    # ✅ Corrected keyword argument name\n",
    "    # data_conn = DataConnector(logger,'creds.json',data_dir_in,use_local_data=True,local_data_dir_out=data_dir_out)\n",
    "\n",
    "    # claims = data_conn.read_data('claim_rollup', csv=True)\n",
    "    # cost = data_conn.read_data('case_bill', csv=True)\n",
    "    # claim_rollup_mapping = data_conn.read_data('claim_rollup_mapping', csv=True)\n",
    "\n",
    "    \n",
    "    # ✅ Re-read with header=0 to ensure proper column names\n",
    "    claims = pd.read_csv(f\"{data_dir_in}/claim_rollup.csv\", header=1, encoding='utf-8-sig')\n",
    "    cost = pd.read_csv(f\"{data_dir_in}/case_bill.csv\", header=1, encoding='utf-8-sig')\n",
    "    claim_rollup_mapping = pd.read_csv(f\"{data_dir_in}/claim_rollup_mapping.csv\", header=1, encoding='utf-8-sig')\n",
    "\n",
    "    print(\"Claims shape:\", claims.shape)\n",
    "    print(\"Cost shape:\", cost.shape)\n",
    "    print(\"Claim Rollup Mapping shape:\", claim_rollup_mapping.shape)\n",
    "\n",
    "    # data_conn.close_connections()\n",
    "\n",
    "    return claims, cost, claim_rollup_mapping, logger, data_dir_out\n",
    "\n",
    "# ✅ Call the function and unpack the results\n",
    "claims, cost, claim_rollup_mapping, logger, data_dir_out = test_data_loading()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "efc37f82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Column",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Non-Null Count",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Dtype",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "d954aef1-9aa7-43ee-8c4a-c577a1ff76ff",
       "rows": [
        [
         "0",
         "index",
         "47311",
         "int64"
        ],
        [
         "1",
         "ClaimNo",
         "47311",
         "object"
        ],
        [
         "2",
         "n_cases",
         "47311",
         "int64"
        ],
        [
         "3",
         "first_referral",
         "47311",
         "object"
        ],
        [
         "4",
         "has_non_assessment",
         "47311",
         "bool"
        ],
        [
         "5",
         "DateOfReferralFirst",
         "47311",
         "object"
        ],
        [
         "6",
         "DateOfReferralLast",
         "47311",
         "object"
        ],
        [
         "7",
         "rtw_days_max",
         "43179",
         "float64"
        ],
        [
         "8",
         "outcome_excluded_min",
         "47311",
         "bool"
        ],
        [
         "9",
         "category_included_max",
         "47311",
         "float64"
        ],
        [
         "10",
         "capacity_first_notnull",
         "24233",
         "object"
        ],
        [
         "11",
         "capacity_type_first_notnull",
         "22925",
         "object"
        ],
        [
         "12",
         "capacity_weight_first_notnull",
         "21295",
         "float64"
        ],
        [
         "13",
         "industry_first_notnull",
         "22800",
         "object"
        ],
        [
         "14",
         "Industry_physicality_first_notnull",
         "22799",
         "float64"
        ],
        [
         "15",
         "injury_first_notnull",
         "22565",
         "object"
        ],
        [
         "16",
         "injury_type_first_notnull",
         "22565",
         "object"
        ],
        [
         "17",
         "injury_number_first_notnull",
         "15446",
         "float64"
        ],
        [
         "18",
         "injury_group_first_notnull",
         "22565",
         "object"
        ],
        [
         "19",
         "injury_group_number_first_notnull",
         "22565",
         "float64"
        ],
        [
         "20",
         "DateOfInjury_min",
         "39588",
         "object"
        ],
        [
         "21",
         "CostsTotalExTax",
         "47311",
         "float64"
        ],
        [
         "22",
         "Duration",
         "47311",
         "float64"
        ],
        [
         "23",
         "case_sources",
         "47311",
         "object"
        ],
        [
         "24",
         "days_btwn_refs",
         "47311",
         "int64"
        ],
        [
         "25",
         "days_btwn_refs_30daybin",
         "47311",
         "int64"
        ],
        [
         "26",
         "Cause",
         "24835",
         "object"
        ],
        [
         "27",
         "Gender",
         "28002",
         "object"
        ],
        [
         "28",
         "PostalCode",
         "27626",
         "float64"
        ],
        [
         "29",
         "State",
         "29538",
         "object"
        ],
        [
         "30",
         "age",
         "27668",
         "float64"
        ],
        [
         "31",
         "agent_qual_grp_first",
         "47311",
         "object"
        ],
        [
         "32",
         "industry_first",
         "22157",
         "object"
        ],
        [
         "33",
         "Industry_physicality_first",
         "22157",
         "float64"
        ],
        [
         "34",
         "injury_first",
         "21932",
         "object"
        ],
        [
         "35",
         "injury_type_first",
         "21932",
         "object"
        ],
        [
         "36",
         "injury_number_first",
         "14935",
         "float64"
        ],
        [
         "37",
         "injury_group_first",
         "21932",
         "object"
        ],
        [
         "38",
         "injury_group_number_first",
         "21932",
         "float64"
        ],
        [
         "39",
         "capacity_first",
         "23536",
         "object"
        ],
        [
         "40",
         "capacity_type_first",
         "22254",
         "object"
        ],
        [
         "41",
         "capacity_weight_first",
         "20640",
         "float64"
        ],
        [
         "42",
         "outcome_first",
         "42668",
         "object"
        ],
        [
         "43",
         "outcome_type_first",
         "23496",
         "object"
        ],
        [
         "44",
         "TeamName_first",
         "28607",
         "object"
        ],
        [
         "45",
         "BillTo_first",
         "23811",
         "object"
        ],
        [
         "46",
         "agent_qual_grp_last",
         "47311",
         "object"
        ],
        [
         "47",
         "DateClosedLast",
         "42975",
         "object"
        ],
        [
         "48",
         "rtw_days_last",
         "42975",
         "float64"
        ],
        [
         "49",
         "req_type_last",
         "29515",
         "object"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 68
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Column</th>\n",
       "      <th>Non-Null Count</th>\n",
       "      <th>Dtype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>index</td>\n",
       "      <td>47311</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ClaimNo</td>\n",
       "      <td>47311</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>n_cases</td>\n",
       "      <td>47311</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>first_referral</td>\n",
       "      <td>47311</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>has_non_assessment</td>\n",
       "      <td>47311</td>\n",
       "      <td>bool</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>BillTo_last</td>\n",
       "      <td>24864</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>BillToId_last</td>\n",
       "      <td>29831</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>CaseTypeId_last</td>\n",
       "      <td>30012</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>BusinessUnitId_last</td>\n",
       "      <td>30012</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>days_to_first_referral</td>\n",
       "      <td>39588</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>68 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Column Non-Null Count    Dtype\n",
       "0                    index          47311    int64\n",
       "1                  ClaimNo          47311   object\n",
       "2                  n_cases          47311    int64\n",
       "3           first_referral          47311   object\n",
       "4       has_non_assessment          47311     bool\n",
       "..                     ...            ...      ...\n",
       "63             BillTo_last          24864   object\n",
       "64           BillToId_last          29831  float64\n",
       "65         CaseTypeId_last          30012  float64\n",
       "66     BusinessUnitId_last          30012  float64\n",
       "67  days_to_first_referral          39588  float64\n",
       "\n",
       "[68 rows x 3 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert claims.info() into dataframe as so many columns :\n",
    "buffer = io.StringIO()\n",
    "claims.info(buf=buffer)\n",
    "lines = buffer.getvalue().splitlines()\n",
    "\n",
    "data = []\n",
    "for line in lines:\n",
    "    parts = line.split()\n",
    "    if len(parts) >= 4 and parts[0].isdigit():\n",
    "        col_name = parts[1]\n",
    "        non_null = parts[2]\n",
    "        dtype = parts[-1]\n",
    "        data.append([col_name, non_null, dtype])\n",
    "\n",
    "info_df = pd.DataFrame(data, columns=['Column', 'Non-Null Count', 'Dtype'])\n",
    "info_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "942e32d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cost['CostsTotalExTax'] = cost['CostsTotalExTax'].fillna(0)\n",
    "cost['CostsTotalExTax'] = cost['Duration'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d035299c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95m 2025-08-07 16:30:07 DEBUG Categorising activities \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "cost_ca = categorise_activities(logger, cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d01cf6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "claims_cost = claim_rollup_mapping.merge(cost_ca, how='left', on=['CaseServiceId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5497cdc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m 2025-08-07 16:30:32 DEBUG Removing claims that have fixed-fee costs \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "claim_totals = get_claim_totals(logger, claims_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d7ae31cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95m 2025-08-07 16:30:35 DEBUG Getting weekly estimates \u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_46939/1306210536.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  claims_df['first_referral'] = pd.to_datetime(claims_df['first_referral'])\n",
      "/tmp/ipykernel_46939/1306210536.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  claims_df['date_closed'] = pd.to_datetime(claims_df['DateClosedLast'])\n",
      "/tmp/ipykernel_46939/1306210536.py:52: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  claims_df['date_closed'] = pd.to_datetime(claims_df['date_closed'].fillna(datetime.date.today()))\n",
      "/tmp/ipykernel_46939/1306210536.py:53: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  claims_df['diff_days'] = (claims_df['date_closed'] - claims_df['first_referral']).dt.days\n",
      "/tmp/ipykernel_46939/1306210536.py:54: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  claims_df['week_num_max'] = (np.floor(claims_df['diff_days']/7)+1).astype('int')\n"
     ]
    }
   ],
   "source": [
    "cost_weekly = get_weekly_estimates(logger, claims_cost, claims, exclude_travel)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "10c87f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95m 2025-08-07 16:30:50 DEBUG Generating milestones \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "milestones = generate_milestones(logger, cost_weekly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "20090fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95m 2025-08-07 16:31:00 DEBUG Generating milestones \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# 保存 claim_totals\n",
    "claim_totals.to_csv(f'{data_dir_out}/claim_cost_totals.csv', index=False)\n",
    "\n",
    "# 根据 exclude_travel 设置文件名\n",
    "suffix = '_ex_travel' if exclude_travel else ''\n",
    "cost_weekly.to_csv(f'{data_dir_out}/cost_weekly{suffix}.csv', index=False)\n",
    "\n",
    "# 生成并保存 milestones\n",
    "milestones = generate_milestones(logger, cost_weekly)\n",
    "milestones.to_csv(f'{data_dir_out}/milestones{suffix}.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4fa14ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95m 2025-08-07 16:31:05 DEBUG Completed lifcycle dataprep \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "logger.debug('Completed lifcycle dataprep', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3b6325",
   "metadata": {},
   "source": [
    "## below script is to write-back to datalake :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "158216e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'date': datetime.datetime(2025, 8, 7, 6, 31, 57, tzinfo=datetime.timezone.utc),\n",
       " 'etag': '\"0x8DDD57C1C0B8E34\"',\n",
       " 'last_modified': datetime.datetime(2025, 8, 7, 6, 31, 57, tzinfo=datetime.timezone.utc),\n",
       " 'content_length': 0,\n",
       " 'client_request_id': '37d3f56e-7358-11f0-bcc0-6045bde48a8f',\n",
       " 'request_id': 'd36e32f5-a01f-0008-0f64-07fea9000000',\n",
       " 'version': '2025-05-05',\n",
       " 'request_server_encrypted': False,\n",
       " 'encryption_key_sha256': None,\n",
       " 'lease_renewed': None}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azure.storage.filedatalake import DataLakeServiceClient\n",
    "\n",
    "base_path = \"/home/azureuser/mycode/gitrepos/AzureMachineLearning/py_for_da_course\"  # Leave as \"\" if file is in current folder\n",
    "csv_filename = \"ab_query_type_df.csv\"\n",
    "csv_filepath = os.path.join(base_path, csv_filename)\n",
    "prompt_name= csv_filename\n",
    "with open(csv_filepath, 'r', encoding='utf-8') as f:\n",
    "    csv_data = f.read()\n",
    "\n",
    "# import io\n",
    "# csv_buffer = io.StringIO()\n",
    "# df.to_csv(csv_buffer, index=False)\n",
    "# csv_data = csv_buffer.getvalue()\n",
    "\n",
    "connection_string = \"DefaultEndpointsProtocol=https;AccountName=stdataanalyticsadls001;AccountKey=ncWVOLxWUDzS9+6jmziedHHr1lXV1xTwr2B4oK58Q7osopvYepDuJbUb7drz4RPgMrzfPQEVhl+nuKtgNHwCDQ==;EndpointSuffix=core.windows.net\"\n",
    "service_client = DataLakeServiceClient.from_connection_string(connection_string)\n",
    "file_system_client = service_client.get_file_system_client(\"datalakecontainer\")\n",
    "directory_client = file_system_client.get_directory_client(\"Rehab/lifecycle_\")\n",
    "file_client = directory_client.create_file(f\"writeback_{prompt_name}.csv\")\n",
    "file_client.append_data(data=csv_data, offset=0, length=len(csv_data))\n",
    "file_client.flush_data(len(csv_data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azureml_py310_sdkv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
